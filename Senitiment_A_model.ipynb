{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON data using Pandas\n",
    "data_1 = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data_2 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the data into a single DataFrame\n",
    "data = pd.concat([data_1, data_2])\n",
    "data.head()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "data = data.drop_duplicates()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x: x[0] != '@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "    text = emoji.sub(r'', text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean text\n",
    "def clean_tokenize(df):\n",
    "    head_lines = []\n",
    "    lines = df[\"headline\"].values.tolist()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for line in lines:\n",
    "        line = clean_text(line)\n",
    "        tokens = word_tokenize(line)\n",
    "        words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        head_lines.append(words)\n",
    "        \n",
    "    return head_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and preprocess data\n",
    "head_lines = clean_tokenize(data)\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(head_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(head_lines)\n",
    "word_index = tokenizer_obj.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "max_length = 25\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "sentiment = data['is_sarcastic'].values\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2  4824  3346 ...     0     0     0]\n",
      " [  238   648  6049 ...     0     0     0]\n",
      " [ 9881   409   583 ...     0     0     0]\n",
      " ...\n",
      " [14506   792   304 ...     0     0     0]\n",
      " [ 1537  3526   270 ...     0     0     0]\n",
      " [   57  1413 16295 ...     0     0     0]] [1 1 0 ... 1 1 1]\n",
      "[[ 6262    74     3 ...     0     0     0]\n",
      " [    2   719 12151 ...     0     0     0]\n",
      " [   35  4253   762 ...     0     0     0]\n",
      " ...\n",
      " [ 7457   647  2058 ...     0     0     0]\n",
      " [ 2787   587  2259 ...     0     0     0]\n",
      " [  144   391  1676 ...     0     0     0]] [1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split data\n",
    "indices = np.arange(lines_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "lines_pad = lines_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "validation_split = 0.3\n",
    "num_validation_samples = int(validation_split * lines_pad.shape[0])\n",
    "X_train_pad = lines_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = lines_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "print(X_train_pad,y_train)\n",
    "print(X_test_pad,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "embedding_dim = 100\n",
    "GLOVE_DIR = \"D:\\ML\\content\"\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.100d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create embedding layer\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.25, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model by providing a sample input\n",
    "model.build(input_shape=(None, max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,856,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │     \u001b[38;5;34m2,856,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,973,977</span> (11.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,973,977\u001b[0m (11.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,377\u001b[0m (458.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,856,600</span> (10.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,856,600\u001b[0m (10.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - acc: 0.6620 - loss: 0.6106 - val_acc: 0.7486 - val_loss: 0.5160\n",
      "Epoch 2/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - acc: 0.7428 - loss: 0.5278 - val_acc: 0.7800 - val_loss: 0.4722\n",
      "Epoch 3/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.7663 - loss: 0.4908 - val_acc: 0.7877 - val_loss: 0.4576\n",
      "Epoch 4/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.7804 - loss: 0.4654 - val_acc: 0.7958 - val_loss: 0.4382\n",
      "Epoch 5/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - acc: 0.7920 - loss: 0.4494 - val_acc: 0.8093 - val_loss: 0.4159\n",
      "Epoch 6/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.8013 - loss: 0.4283 - val_acc: 0.8112 - val_loss: 0.4117\n",
      "Epoch 7/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.8084 - loss: 0.4145 - val_acc: 0.8136 - val_loss: 0.4153\n",
      "Epoch 8/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.8125 - loss: 0.4061 - val_acc: 0.8141 - val_loss: 0.4014\n",
      "Epoch 9/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - acc: 0.8252 - loss: 0.3900 - val_acc: 0.8202 - val_loss: 0.3933\n",
      "Epoch 10/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - acc: 0.8300 - loss: 0.3751 - val_acc: 0.8207 - val_loss: 0.3895\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('sentiment_sarcasm_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and clean the input text\n",
    "def preprocess_input(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment and sarcasm\n",
    "def predict_sentiment_and_sarcasm(text, model, tokenizer, max_length):\n",
    "    words = preprocess_input(text)\n",
    "    sequences = tokenizer.texts_to_sequences([words])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequences)\n",
    "    sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "    sarcasm = 'Sarcastic' if prediction >= 0.5 else 'Not Sarcastic'\n",
    "    return sentiment, sarcasm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n"
     ]
    }
   ],
   "source": [
    "# Test the model with an example input\n",
    "test_input = \"richard branson's global-warming donation nearly as much as cost of failed balloon trips\"\n",
    "sentiment, sarcasm = predict_sentiment_and_sarcasm(test_input, model, tokenizer_obj, max_length)\n",
    "\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Sarcasm: {sarcasm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n"
     ]
    }
   ],
   "source": [
    "# Test the model with an example input\n",
    "test_input = \"richard branson's global-warming donation nearly as much as cost of failed balloon trips😨😰😥😓\"\n",
    "sentiment, sarcasm = predict_sentiment_and_sarcasm(test_input, model, tokenizer_obj, max_length)\n",
    "\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Sarcasm: {sarcasm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Sentence Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n",
      "Emoji Sentiments: Neutral\n",
      "Emoji Counts: {'positive_count': 0, 'negative_count': 0, 'neutral_count': 4}\n",
      "Final Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import emoji\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('sentiment_sarcasm_model.h5')\n",
    "\n",
    "# Initialize tokenizer (ensure it matches the one used during training)\n",
    "def initialize_tokenizer():\n",
    "    # Example tokenization initialization. Replace with actual tokenizer if saved and loaded.\n",
    "    return Tokenizer()\n",
    "\n",
    "tokenizer_obj = initialize_tokenizer()\n",
    "\n",
    "# Define emoji mappings\n",
    "positive_emojis = [\"😀\", \"😃\", \"😄\", \"😁\", \"😆\", \"😅\", \"😂\", \"🤣\", \"😊\", \"😇\", \"🙂\", \"😉\", \"😌\", \"😍\", \"🥰\", \"😘\", \"😗\", \"😙\", \"😚\", \"😋\", \"😛\", \"😝\", \"😜\", \"🤪\", \"😎\", \"🤩\", \"🥳\", \"😏\", \"😬\", \"🤗\"]\n",
    "negative_emojis = [\"🥹\", \"🥲\", \"☺️\", \"😐\", \"😑\", \"😶\", \"🙃\", \"😶‍🌫\", \"🤔\", \"🫣\", \"🤭\", \"🫡\", \"🫢\", \"🫡\", \"🤫\", \"🫠\", \"🤥\", \"😶\", \"🫥\", \"😐\", \"🫤\", \"😑\", \"🫨\", \"🙄\", \"😯\", \"😦\", \"😧\", \"😮\", \"😲\", \"🥱\", \"😴\", \"🤤\", \"😪\", \"😵\", \"🤐\", \"🥴\", \"🤢\", \"🤧\", \"😷\", \"🤒\", \"🤕\", \"🤑\", \"🤠\"]\n",
    "neutral_emojis = [\"😞\", \"😔\", \"😟\", \"😕\", \"🙁\", \"☹️\", \"😣\", \"😖\", \"😫\", \"😩\", \"🥺\", \"😢\", \"😭\", \"😤\", \"😠\", \"😡\", \"🤬\", \"🤯\", \"😳\", \"🥵\", \"🥶\", \"😱\", \"😨\", \"😰\", \"😥\", \"😓\", \"😞\", \"😧\", \"😦\", \"😈\", \"👿\", \"👹\", \"👺\", \"💩\", \"😵\", \"😿\"]\n",
    "\n",
    "# Define functions for sentiment analysis\n",
    "def detect_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "def classify_emojis(emojis):\n",
    "    positive_count = sum(1 for e in emojis if e in positive_emojis)\n",
    "    negative_count = sum(1 for e in emojis if e in negative_emojis)\n",
    "    neutral_count = sum(1 for e in emojis if e in neutral_emojis)\n",
    "    \n",
    "    return {\n",
    "        \"positive_count\": positive_count,\n",
    "        \"negative_count\": negative_count,\n",
    "        \"neutral_count\": neutral_count\n",
    "    }\n",
    "\n",
    "def preprocess_input(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    padded_input = pad_sequences(tokens, maxlen=max_length, padding='post')\n",
    "    return padded_input\n",
    "\n",
    "def predict_sarcasm(text, model, tokenizer, max_length):\n",
    "    padded_input = preprocess_input(text, tokenizer, max_length)\n",
    "    prediction = model.predict(padded_input)\n",
    "    return 'Sarcastic' if prediction >= 0.5 else 'Not Sarcastic'\n",
    "\n",
    "def predict_sentence_sentiment(text, model, tokenizer, max_length):\n",
    "    padded_input = preprocess_input(text, tokenizer, max_length)\n",
    "    prediction = model.predict(padded_input)\n",
    "    return 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "\n",
    "def get_final_sentiment(text, model, tokenizer, max_length):\n",
    "    detected_emojis = detect_emojis(text)\n",
    "    emoji_counts = classify_emojis(detected_emojis)\n",
    "    \n",
    "    sarcasm = predict_sarcasm(text, model, tokenizer, max_length)\n",
    "    sentence_sentiment = predict_sentence_sentiment(text, model, tokenizer, max_length)\n",
    "    \n",
    "    # Determine emoji sentiment\n",
    "    if emoji_counts['positive_count'] > emoji_counts['negative_count']:\n",
    "        emoji_sentiment = 'Positive'\n",
    "    elif emoji_counts['negative_count'] > emoji_counts['positive_count']:\n",
    "        emoji_sentiment = 'Negative'\n",
    "    else:\n",
    "        emoji_sentiment = 'Neutral'\n",
    "    \n",
    "    # Combine results\n",
    "    if sarcasm == 'Sarcastic':\n",
    "        final_sentiment = 'Sarcastic'\n",
    "    elif emoji_sentiment == 'Positive':\n",
    "        final_sentiment = 'Positive'\n",
    "    elif emoji_sentiment == 'Negative':\n",
    "        final_sentiment = 'Negative'\n",
    "    else:\n",
    "        final_sentiment = sentence_sentiment\n",
    "    \n",
    "    return {\n",
    "        'sentence_sentiment': sentence_sentiment,\n",
    "        'sarcasm': sarcasm,\n",
    "        'emoji_sentiment': emoji_sentiment,\n",
    "        'emoji_counts': emoji_counts,\n",
    "        'final_sentiment': final_sentiment\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "text = \"richard branson's global-warming donation nearly as much as cost of failed balloon trips😨😰😥😓\"\n",
    "final_results = get_final_sentiment(text, model, tokenizer_obj, max_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sentence Sentiment: {final_results['sentence_sentiment']}\")\n",
    "print(f\"Sarcasm: {final_results['sarcasm']}\")\n",
    "print(f\"Emoji Sentiments: {final_results['emoji_sentiment']}\")\n",
    "print(f\"Emoji Counts: {final_results['emoji_counts']}\")\n",
    "print(f\"Final Sentiment: {final_results['final_sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Sentence Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n",
      "Emoji Sentiments: Neutral\n",
      "Emoji Counts: {'positive_count': 0, 'negative_count': 0, 'neutral_count': 0}\n",
      "Final Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "text = \"Oh, great. Another Monday. I was just hoping for a reason to hate my life more\"\n",
    "final_results = get_final_sentiment(text, model, tokenizer_obj, max_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sentence Sentiment: {final_results['sentence_sentiment']}\")\n",
    "print(f\"Sarcasm: {final_results['sarcasm']}\")\n",
    "print(f\"Emoji Sentiments: {final_results['emoji_sentiment']}\")\n",
    "print(f\"Emoji Counts: {final_results['emoji_counts']}\")\n",
    "print(f\"Final Sentiment: {final_results['final_sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Sentence Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n",
      "Emoji Sentiments: Neutral\n",
      "Emoji Counts: {'positive_count': 0, 'negative_count': 0, 'neutral_count': 0}\n",
      "Final Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "text = \"frustrated jesus christ forced to find 22nd vessel for reincarnation after death of charles manson\"\n",
    "final_results = get_final_sentiment(text, model, tokenizer_obj, max_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sentence Sentiment: {final_results['sentence_sentiment']}\")\n",
    "print(f\"Sarcasm: {final_results['sarcasm']}\")\n",
    "print(f\"Emoji Sentiments: {final_results['emoji_sentiment']}\")\n",
    "print(f\"Emoji Counts: {final_results['emoji_counts']}\")\n",
    "print(f\"Final Sentiment: {final_results['final_sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Sentence Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n",
      "Emoji Sentiments: Neutral\n",
      "Emoji Counts: {'positive_count': 0, 'negative_count': 0, 'neutral_count': 4}\n",
      "Final Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import emoji\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('sentiment_sarcasm_model.h5')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "\n",
    "# Initialize tokenizer (ensure it matches the one used during training)\n",
    "def initialize_tokenizer():\n",
    "    # Load your tokenizer here\n",
    "    tokenizer = Tokenizer()\n",
    "    # Example of loading saved tokenizer\n",
    "    # with open('tokenizer.pickle', 'rb') as handle:\n",
    "    #     tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_obj = initialize_tokenizer()\n",
    "\n",
    "# Define emoji mappings\n",
    "positive_emojis = [\"😀\", \"😃\", \"😄\", \"😁\", \"😆\", \"😅\", \"😂\", \"🤣\", \"😊\", \"😇\", \"🙂\", \"😉\", \"😌\", \"😍\", \"🥰\", \"😘\", \"😗\", \"😙\", \"😚\", \"😋\", \"😛\", \"😝\", \"😜\", \"🤪\", \"😎\", \"🤩\", \"🥳\", \"😏\", \"😬\", \"🤗\"]\n",
    "negative_emojis = [\"🥹\", \"🥲\", \"☺️\", \"😐\", \"😑\", \"😶\", \"🙃\", \"😶‍🌫\", \"🤔\", \"🫣\", \"🤭\", \"🫡\", \"🫢\", \"🫡\", \"🤫\", \"🫠\", \"🤥\", \"😶\", \"🫥\", \"😐\", \"🫤\", \"😑\", \"🫨\", \"🙄\", \"😯\", \"😦\", \"😧\", \"😮\", \"😲\", \"🥱\", \"😴\", \"🤤\", \"😪\", \"😵\", \"🤐\", \"🥴\", \"🤢\", \"🤧\", \"😷\", \"🤒\", \"🤕\", \"🤑\", \"🤠\"]\n",
    "neutral_emojis = [\"😞\", \"😔\", \"😟\", \"😕\", \"🙁\", \"☹️\", \"😣\", \"😖\", \"😫\", \"😩\", \"🥺\", \"😢\", \"😭\", \"😤\", \"😠\", \"😡\", \"🤬\", \"🤯\", \"😳\", \"🥵\", \"🥶\", \"😱\", \"😨\", \"😰\", \"😥\", \"😓\", \"😞\", \"😧\", \"😦\", \"😈\", \"👿\", \"👹\", \"👺\", \"💩\", \"😵\", \"😿\"]\n",
    "\n",
    "# Define functions for sentiment analysis\n",
    "def detect_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "def classify_emojis(emojis):\n",
    "    positive_count = sum(1 for e in emojis if e in positive_emojis)\n",
    "    negative_count = sum(1 for e in emojis if e in negative_emojis)\n",
    "    neutral_count = sum(1 for e in emojis if e in neutral_emojis)\n",
    "    \n",
    "    return {\n",
    "        \"positive_count\": positive_count,\n",
    "        \"negative_count\": negative_count,\n",
    "        \"neutral_count\": neutral_count\n",
    "    }\n",
    "\n",
    "def preprocess_input(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    padded_input = pad_sequences(tokens, maxlen=max_length, padding='post')\n",
    "    return padded_input\n",
    "\n",
    "def predict_sarcasm(text, model, tokenizer, max_length):\n",
    "    padded_input = preprocess_input(text, tokenizer, max_length)\n",
    "    prediction = model.predict(padded_input)\n",
    "    # Adjust logic according to the shape of prediction\n",
    "    if len(prediction[0]) == 2:  # Binary classification\n",
    "        return 'Sarcastic' if prediction[0][1] > prediction[0][0] else 'Not Sarcastic'\n",
    "    else:\n",
    "        return 'Not Sarcastic'  # Default case, if shape is not as expected\n",
    "\n",
    "def predict_sentence_sentiment(text, model, tokenizer, max_length):\n",
    "    padded_input = preprocess_input(text, tokenizer, max_length)\n",
    "    prediction = model.predict(padded_input)\n",
    "    # Adjust logic according to the shape of prediction\n",
    "    if len(prediction[0]) == 3:  # Assuming model has 3 outputs for sentiment classification\n",
    "        return 'Positive' if prediction[0][2] > prediction[0][1] and prediction[0][2] > prediction[0][0] else 'Negative'\n",
    "    else:\n",
    "        return 'Negative'  # Default case, if shape is not as expected\n",
    "\n",
    "def get_final_sentiment(text, model, tokenizer, max_length):\n",
    "    detected_emojis = detect_emojis(text)\n",
    "    emoji_counts = classify_emojis(detected_emojis)\n",
    "    \n",
    "    sarcasm = predict_sarcasm(text, model, tokenizer, max_length)\n",
    "    sentence_sentiment = predict_sentence_sentiment(text, model, tokenizer, max_length)\n",
    "    \n",
    "    # Determine emoji sentiment\n",
    "    if emoji_counts['positive_count'] > emoji_counts['negative_count']:\n",
    "        emoji_sentiment = 'Positive'\n",
    "    elif emoji_counts['negative_count'] > emoji_counts['positive_count']:\n",
    "        emoji_sentiment = 'Negative'\n",
    "    else:\n",
    "        emoji_sentiment = 'Neutral'\n",
    "    \n",
    "    # Combine results\n",
    "    if sarcasm == 'Sarcastic':\n",
    "        final_sentiment = 'Sarcastic'\n",
    "    elif emoji_sentiment == 'Positive':\n",
    "        final_sentiment = 'Positive'\n",
    "    elif emoji_sentiment == 'Negative':\n",
    "        final_sentiment = 'Negative'\n",
    "    else:\n",
    "        final_sentiment = sentence_sentiment\n",
    "    \n",
    "    return {\n",
    "        'sentence_sentiment': sentence_sentiment,\n",
    "        'sarcasm': sarcasm,\n",
    "        'emoji_sentiment': emoji_sentiment,\n",
    "        'emoji_counts': emoji_counts,\n",
    "        'final_sentiment': final_sentiment\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "text = \"richard branson's global-warming donation nearly as much as cost of failed balloon trips😨😰😥😓\"\n",
    "max_length = 100  # Example max length, adjust to your model's requirements\n",
    "final_results = get_final_sentiment(text, model, tokenizer_obj, max_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sentence Sentiment: {final_results['sentence_sentiment']}\")\n",
    "print(f\"Sarcasm: {final_results['sarcasm']}\")\n",
    "print(f\"Emoji Sentiments: {final_results['emoji_sentiment']}\")\n",
    "print(f\"Emoji Counts: {final_results['emoji_counts']}\")\n",
    "print(f\"Final Sentiment: {final_results['final_sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "Sentence Sentiment: Negative\n",
      "Sarcasm: Not Sarcastic\n",
      "Emoji Sentiment: Negative\n",
      "Punctuation Sentiment: Neutral\n",
      "Final Sentiment: Slightly Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define emoji mappings\n",
    "positive_emojis = [\"😀\", \"😃\", \"😄\", \"😁\", \"😆\", \"😅\", \"😂\", \"🤣\", \"😊\", \"😇\", \"🙂\", \"😉\", \"😌\", \"😍\", \"🥰\", \"😘\", \"😗\", \"😙\", \"😚\", \"😋\", \"😛\", \"😝\", \"😜\", \"🤪\", \"😎\", \"🤩\", \"🥳\", \"😏\", \"😬\", \"🤗\"]\n",
    "negative_emojis = [\"😞\", \"😔\", \"😟\", \"😕\", \"🙁\", \"☹️\", \"😣\", \"😖\", \"😫\", \"😩\", \"🥺\", \"😢\", \"😭\", \"😤\", \"😠\", \"😡\", \"🤬\", \"🤯\", \"😳\", \"🥵\", \"🥶\", \"😱\", \"😨\", \"😰\", \"😥\", \"😓\", \"😈\", \"👿\", \"👹\", \"👺\", \"💩\", \"😿\"]\n",
    "neutral_emojis = [\"🥹\", \"🥲\", \"☺️\", \"😐\", \"😑\", \"😶\", \"🙃\", \"😶‍🌫\", \"🤔\", \"🫣\", \"🤭\", \"🫡\", \"🫢\", \"🫡\", \"🤫\", \"🫠\", \"🤥\", \"😶\", \"🫥\", \"😐\", \"🫤\", \"😑\", \"🫨\", \"🙄\", \"😯\", \"😦\", \"😧\", \"😮\", \"😲\", \"🥱\", \"😴\", \"🤤\", \"😪\", \"😵\", \"🤐\", \"🥴\", \"🤢\", \"🤧\", \"😷\", \"🤒\", \"🤕\", \"🤑\", \"🤠\"]\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Tokenize and clean text\n",
    "def clean_tokenize(df):\n",
    "    head_lines = []\n",
    "    lines = df[\"headline\"].values.tolist()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for line in lines:\n",
    "        line = clean_text(line)\n",
    "        tokens = word_tokenize(line)\n",
    "        words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        head_lines.append(words)\n",
    "        \n",
    "    return head_lines\n",
    "\n",
    "# Define emoji and punctuation sentiment analysis function\n",
    "def analyze_emoji_punctuation(text):\n",
    "    emojis = ''.join(c for c in text if c in emoji.EMOJI_DATA)\n",
    "    punctuations = ''.join(c for c in text if c in string.punctuation)\n",
    "    \n",
    "    emoji_counts = {\n",
    "        'positive_count': sum(1 for e in emojis if e in positive_emojis),\n",
    "        'negative_count': sum(1 for e in emojis if e in negative_emojis),\n",
    "        'neutral_count': sum(1 for e in emojis if e in neutral_emojis)\n",
    "    }\n",
    "    \n",
    "    # Determine emoji sentiment\n",
    "    if emoji_counts['positive_count'] > emoji_counts['negative_count']:\n",
    "        emoji_sentiment = 'Positive'\n",
    "    elif emoji_counts['negative_count'] > emoji_counts['positive_count']:\n",
    "        emoji_sentiment = 'Negative'\n",
    "    else:\n",
    "        emoji_sentiment = 'Neutral'\n",
    "    \n",
    "    if '!' in punctuations or '!!!' in punctuations:\n",
    "        punctuation_sentiment = 'Positive'\n",
    "    elif '...' in punctuations or '??' in punctuations:\n",
    "        punctuation_sentiment = 'Negative'\n",
    "    else:\n",
    "        punctuation_sentiment = 'Neutral'\n",
    "    \n",
    "    return emoji_sentiment, punctuation_sentiment\n",
    "\n",
    "# Define model prediction functions\n",
    "def predict_sarcasm_sentiment(text, model, tokenizer, max_length):\n",
    "    padded_input = preprocess_input(text, tokenizer, max_length)\n",
    "    prediction = model.predict(padded_input)\n",
    "    sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "    sarcasm = 'Sarcastic' if prediction >= 0.5 else 'Not Sarcastic'\n",
    "    return sentiment, sarcasm\n",
    "\n",
    "def preprocess_input(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    padded_input = pad_sequences(tokens, maxlen=max_length, padding='post')\n",
    "    return padded_input\n",
    "\n",
    "# Define final sentiment analysis function\n",
    "def get_final_sentiment(text, model, tokenizer, max_length):\n",
    "    # Detect emojis and classify sentiment using heuristics\n",
    "    emoji_sentiment, punctuation_sentiment = analyze_emoji_punctuation(text)\n",
    "    \n",
    "    # Predict sarcasm and sentence sentiment using the model\n",
    "    sentence_sentiment, sarcasm = predict_sarcasm_sentiment(text, model, tokenizer, max_length)\n",
    "    \n",
    "    # Combine results logically\n",
    "    if sarcasm == 'Sarcastic':\n",
    "        if emoji_sentiment == 'Positive' and punctuation_sentiment == 'Positive':\n",
    "            final_sentiment = 'Sarcastic and Positive'\n",
    "        elif emoji_sentiment == 'Negative' and punctuation_sentiment == 'Negative':\n",
    "            final_sentiment = 'Sarcastic and Negative'\n",
    "        elif emoji_sentiment == 'Positive' or punctuation_sentiment == 'Positive':\n",
    "            final_sentiment = 'Sarcastic and Slightly Positive'\n",
    "        elif emoji_sentiment == 'Negative' or punctuation_sentiment == 'Negative':\n",
    "            final_sentiment = 'Sarcastic and Slightly Negative'\n",
    "        else:\n",
    "            final_sentiment = 'Sarcastic'\n",
    "    else:\n",
    "        if emoji_sentiment == 'Positive' and punctuation_sentiment == 'Positive':\n",
    "            final_sentiment = 'Very Positive'\n",
    "        elif emoji_sentiment == 'Negative' and punctuation_sentiment == 'Negative':\n",
    "            final_sentiment = 'Very Negative'\n",
    "        elif emoji_sentiment == 'Positive' or punctuation_sentiment == 'Positive':\n",
    "            final_sentiment = 'Slightly Positive'\n",
    "        elif emoji_sentiment == 'Negative' or punctuation_sentiment == 'Negative':\n",
    "            final_sentiment = 'Slightly Negative'\n",
    "        else:\n",
    "            final_sentiment = sentence_sentiment\n",
    "    \n",
    "    return {\n",
    "        'sentence_sentiment': sentence_sentiment,\n",
    "        'sarcasm': sarcasm,\n",
    "        'emoji_sentiment': emoji_sentiment,\n",
    "        'punctuation_sentiment': punctuation_sentiment,\n",
    "        'final_sentiment': final_sentiment\n",
    "    }\n",
    "\n",
    "# Load data (assuming you have a DataFrame 'data' with 'headline' and 'is_sarcastic' columns)\n",
    "# data = pd.read_csv('your_data.csv')\n",
    "# Read the JSON data using Pandas\n",
    "data_1 = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data_2 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "\n",
    "data = pd.concat([data_1, data_2])\n",
    "data = data.drop_duplicates()\n",
    "#data.head()\n",
    "#len(data)\n",
    "# Tokenize and preprocess data\n",
    "head_lines = clean_tokenize(data)\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(head_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(head_lines)\n",
    "word_index = tokenizer_obj.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "max_length = 25\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "sentiment = data['is_sarcastic'].values\n",
    "print(sentiment)\n",
    "\n",
    "# Example text\n",
    "#text = \"richard branson's global-warming donation nearly as much as cost of failed balloon trips😨😰😥😓😊❤️\"\n",
    "            #Sentence Sentiment: Positive\n",
    "            #Sarcasm: Sarcastic\n",
    "            #Emoji Sentiment: Negative\n",
    "            #Punctuation Sentiment: Neutral\n",
    "            #Final Sentiment: Sarcastic and Slightly Negative\n",
    "\n",
    "text = \"Oh, great. Another Monday. I was just hoping for a reason to hate my life more😣😖\"\n",
    "\n",
    "            # Sentence Sentiment: Negative\n",
    "            # Sarcasm: Not Sarcastic\n",
    "            # Emoji Sentiment: Negative\n",
    "            # Punctuation Sentiment: Neutral\n",
    "            # Final Sentiment: Slightly Negative\n",
    "\n",
    "# Load the model\n",
    "sarcasm_model_path = 'sentiment_sarcasm_model.h5'\n",
    "sarcasm_model = load_model(sarcasm_model_path)\n",
    "\n",
    "# Get final sentiment results\n",
    "final_results = get_final_sentiment(text, sarcasm_model, tokenizer_obj, max_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sentence Sentiment: {final_results['sentence_sentiment']}\")\n",
    "print(f\"Sarcasm: {final_results['sarcasm']}\")\n",
    "print(f\"Emoji Sentiment: {final_results['emoji_sentiment']}\")\n",
    "print(f\"Punctuation Sentiment: {final_results['punctuation_sentiment']}\")\n",
    "print(f\"Final Sentiment: {final_results['final_sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# After fitting tokenizer on texts\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(head_lines)  # Assuming head_lines is your preprocessed text data\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
